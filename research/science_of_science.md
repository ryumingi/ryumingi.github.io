#### The complementary contributions of academia and industry to AI research

Over the last 25 years, industry team get more attention, highly cited, and produces more state of the art models. Academic teams produce higher novelty work, unconvetional and atypical. Robust to controls for subfield, team size, seniority, and prestige. Academic-industry collaboration is more similar to industry teams than academic teams.

Academic-industry team does not bring the "best of the both worlds"

> Is it possible that most academic-industry teams aren't good at working with each other since they focus on different goals and have conflicting values? Also, (Fig.2) shows that academic-industry teams are unusally larger.

The initial high impact of academic-industry team might have been due to larger average team size and self-citation.

In 2020, industry teams were 74% more likely to be highly cited than academic teams.

Industroy or academic-industry team publish on interdisciplinary subfields, which is more likely to be cited by another field.

In AI research, citations are correlated with state of the art models rather than novelty.

> Does the self and corss citations compare between teams or authors? In other words, if an industry team A cites industry team B, is that considered self citation within industry teams?

Larger team size has more influence outlets and likelier to includes researchers with different backgrounds.

> academic-industry team is bigger, then why is it doing worse?

"We speculate that industry research is becoming increasingly more impactful and citation distruptive because they have more access to data, computational power, and people."

"AI field moves too fast"

"Influence of these publications is not perceived through direct citations, but rather through network effects."

> What's the distribution of open access articles among academic and/or industry team?

> Perhaps the issue with academic-industry team is that they have difficulty sharing knowledge. For academic team, they are funded specifically to contribute to scientific community whereas the industry teams have to protect their knowledge since it's a product of the team.

#### Predicting the longevity of resources shared in scientific publications

(First author)

Most important factors are where and how the resource is shared. Author's reputation or prestige of the journal does not play a considerable factor.

Censored regression model, Random Forest, Wayback Machine API.

About 90% of the URLs were alive. The mean life span is 19.45 years.

Self-citation negatively correlates with lifespan. Length of the URL negatively correlates with maintainability. Presence of iframe tag negatively correlates with longevity.

#### The Impact of Heterogeneous Shared Leadership in Scientific Teams

First authors do not fit the definition of scientific leadership.

Combination of senior and junior leaders acn maximize team perforamnce compared to leaders of similar age.

Paper citation is not a perfect metric for evaluating team performance.

#### Effects of Same-Race Mentorship Preferences on Academic Performance and Survival

Increase in same-race mentorship prospensity over the years (last 70 years). It's more pronounced for minorities. High same-race prospensity strongly correlates with lower productivity, impact, and collaboration reach, ultimately leading to 27.6% reduced linklihood of remaining in academia.

> Is there a control for increase in diversity in academic faculty. In other worlds, has there been more same-race pairs in minority because there has been increase in minority faculty and students?

Academic Family Tree project and the MAG.

#### Predicting scientific success

(First author)

academic-tree.org. Scopus. Linear regression with elastic net regularization.

#### Are AI Ethics Conferences Different and More Diverse Compared to Traditional Computer Science Conferences?

(First author)

MAG and Sematic Scholar. BERT-based gender and race prediction model.

> Why does every paper that does gender analysis need to build their own model...

TFIDF + multinomial logistic regression to find predictive tokens.

> This paper is something I've wanted to do with the Open Syllabus dataset. A deep dive into demographics of the academic landscape.

Asian countries lack publications of AIE conferences compared to NA and EU.

AIE has more field diversity but less country diversity. AIE has lower male authorship but higher white authorship. h-index is insignificant.


#### International Workshop on Data-driven Science of Science

"In a practical sense, science of science research can promote young scientists to establish their early career life, better evaluate the performance of scientific projects, track popular research frontiers, and even discover new questions from data." 

> Can we predict who gets tenure and at what year? What are the important factors of such empirically?

#### Is the future of peer review automated?

No.

"Automated screening tools cannot replace peer review, but may aid authors, reviewers, and editors in improving
scientific papers."

ScreenIT pipeline has been used on bioRxiv and medRxiv preprints.

Automated tools hav ethe most potential to aid in assessing compliance.

"Future work should enhance existing tools, simplify integration of tools into editorial systems, and train reviewers, editors and authors to use tool reports to improve papers. If successful, automated tools could reduce poor reporting and educate researchers about reporting best practices."

#### Don't judge a journal by its cover?: Appearance of a Journal's website as predictor of blacklisted Open-Access status

Appearance is a predictive factor of subpar journals (AUC of 0.736). Table of content, social media links and packed content are positive signs of good journals.

Screenshot of 262 journals with 82 un-whitelisted.

> Is this enough data for a CNN model?

> Why not try to build a classifier on a group of publications from the journals?


#### Author-suggested reviewers rate manuscripts much more favorably: A cross-sectional analysis of the neuroscience section of PLOS ONE

(First author)

8K manuscripts from 46K authors and 21K reviwers. author suggested review panel has 20% increase of acceptance compared to editor panels.

